{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "yQaldy8SH6Dl",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amishakapse/Cardio-Vascular-Risk-Pridiction/blob/main/Cardio_Vascular_Risk_Pridiction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Cardio Vascular Risk Pridiction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prevalence of cardiovascular disease is significant worldwide, and timely prediction of cardiovascular risk can aid in prevention and intervention of the disease. In this project, a machine learning model is developed to forecast the ten-year risk of cardiovascular disease in individuals utilizing demographic, clinical, and laboratory data.\n",
        "\n",
        "The Framingham Heart Study dataset, a widely used dataset for cardiovascular risk prediction, is used in this project. The dataset includes information on 3,390 participants who were followed for ten years to track cardiovascular events. It has 17 variables, such as age, sex, blood pressure, cholesterol levels, smoking status, and diabetes status.\n",
        "\n",
        "To develop the model, the data is preprocessed, which involves handling missing values, encoding categorical variables, and scaling numerical variables. After preprocessing, the dataset is divided into training and testing sets, with a 80:20 ratio.\n",
        "\n",
        "Several machine learning algorithms are employed in the training data, such as logistic regression, KNN, XGBoost, SVC, and random forest,as they have demonstrated good results in cardiovascular risk prediction. The algorithms are then assessed on the testing data, using metrics such as accuracy, precision, recall, and AUC-ROC to evaluate the model's performance.\n",
        "\n",
        "The XGBoost algorithm outperforms the others, achieving an accuracy of 0.89, precision of 0.92, recall of 0.85, and AUC-ROC of 0.89. These results suggest that the model performs well in forecasting cardiovascular risk.\n",
        "\n",
        "Further analysis is conducted to identify the most significant features in the dataset. The feature importance plot demonstrates that age, education, prevalentHyp, and cigarettes per day are the most critical features in predicting cardiovascular risk. This knowledge can aid in recognizing high-risk individuals and taking proactive steps.\n",
        "\n",
        "In conclusion, this project demonstrates the effectiveness of machine learning techniques in predicting cardiovascular risk utilizing the Framingham Heart Study dataset. Healthcare professionals may utilize the developed machine learning model to recognize high-risk individuals and take preventative steps to decrease the risk of cardiovascular disease.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cardiovascular disease is a prevalent cause of morbidity and mortality worldwide. Identifying and managing individuals at high risk of developing cardiovascular disease at an early stage is crucial for preventing the disease. However, traditional risk prediction models, such as the Framingham Risk Score, have limitations in their accuracy and do not account for the complex interactions between different risk factors. To improve the accuracy of cardiovascular risk prediction, machine learning techniques have shown promise by integrating various risk factors and identifying non-linear interactions. Nevertheless, there is still a need to develop and validate machine learning models that accurately predict cardiovascular risk by utilizing demographic, clinical, and laboratory data. The objective of this project is to address this need by developing and evaluating a machine learning model for predicting the 10-year risk of cardiovascular disease using the Framingham Heart Study dataset."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from numpy import math\n",
        "\n",
        "# Data Manipulation Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pylab\n",
        "\n",
        "# Pre-processsing Libraries\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Hypothesis testing Library\n",
        "import statsmodels.stats.proportion as smp\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Importing essential libraries to check the accuracy\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "## Warnings \n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A5-rQlOPQ_J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Cardio vascular Risk/data_cardiovascular_risk.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dataset.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "dataset.isna().sum().sort_values(ascending=False).plot(kind ='bar')\n",
        "\n",
        "# Add a title\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Following Information we know from the dataset:\n",
        "\n",
        "1.The Dataset consists of 3390 rows and 17 columns.\n",
        "\n",
        "2.The datatype consists of int, float, and object values.\n",
        "\n",
        "3.There is no duplicate values but there are certain missing values in some variables like glucose, education, BPmeds, totChol, cigssPerDay, BMI, heartRate,\n",
        "   \n",
        "4.'glucose' having the maximum missing values in dataset"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demographic:\n",
        "\n",
        "1.Sex: male or female (\"M\" or \"F\")\n",
        "\n",
        "2.Age: Age of the patient (Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
        "\n",
        "3.Education: The level of education of the patient (categorical values - 1,2,3,4)\n",
        "\n",
        "### Behavioral:\n",
        "\n",
        "1.is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n",
        "\n",
        "2.Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
        "\n",
        "### Medical (history):\n",
        "\n",
        "1.BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
        "\n",
        "2.Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
        "\n",
        "3.Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
        "\n",
        "4.Diabetes: whether or not the patient had diabetes (Nominal)\n",
        "\n",
        "###Medical (current):\n",
        "\n",
        "1.Tot Chol: total cholesterol level (Continuous)\n",
        "\n",
        "2.Sys BP: systolic blood pressure (Continuous)\n",
        "\n",
        "3.Dia BP: diastolic blood pressure (Continuous)\n",
        "\n",
        "4.BMI: Body Mass Index (Continuous)\n",
        "\n",
        "5.Heart Rate: heart rate (Continuous - In medical research, variables such as \n",
        "heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n",
        "\n",
        "6.Glucose: glucose level (Continuous)\n",
        "\n",
        "### Predict variable (desired target):\n",
        "\n",
        "1.10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "dataset.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "data_set = dataset.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Separating the categorical and continous variable and storing them\n",
        "categorical_variable =[]\n",
        "continous_variable =[]\n",
        "\n",
        "for i in data_set.columns:\n",
        "  if i == 'id':\n",
        "    pass\n",
        "  elif data_set[i].nunique() <5:\n",
        "    categorical_variable.append(i)\n",
        "  elif data_set[i].nunique() >= 5:\n",
        "    continous_variable.append(i)\n",
        "\n",
        "print(categorical_variable)\n",
        "print(continous_variable)"
      ],
      "metadata": {
        "id": "ewyJtkGO2np9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Missing Data Percentage')\n",
        "print(round(data_set.isna().sum()[data_set.isna().sum() > 0].sort_values(ascending=False)/len(data_set)*100,2))"
      ],
      "metadata": {
        "id": "Jdy000f7E4UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the column that contains null values  \n",
        "null_column_list= ['glucose','education','BPMeds','totChol','cigsPerDay','BMI','heartRate']\n",
        "# plotting box plot\n",
        "plt.figure(figsize=(15,8))\n",
        "data_set[null_column_list].boxplot()\n",
        "\n"
      ],
      "metadata": {
        "id": "xfhjZDPD5VO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of colors\n",
        "colors = sns.color_palette(\"husl\", len(null_column_list))\n",
        "\n",
        "# Create a figure with 8 subplots (2 rows, 4 columns)\n",
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 10))\n",
        "\n",
        "# Flatten the axes array to make it easier to iterate over\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterate over the null column list and plot each column's distribution\n",
        "for i, column in enumerate(null_column_list):\n",
        "    # Select the current axis\n",
        "    ax = axes[i]\n",
        "    # Plot a distplot of the current column with a different color\n",
        "    sns.distplot(data_set[column], ax=ax, color=colors[i])\n",
        "    # Add a title to the plot\n",
        "    ax.set_title(column)\n",
        "    \n",
        "# Remove any unused subplots\n",
        "for j in range(len(null_column_list), len(axes)):\n",
        "    axes[j].remove()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O_G7hRZpcTnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of a suitable measure of central tendency heavily relies on the characteristics of the data under consideration. Normally, when the data is continuous, and follows a normal distribution without any outliers, the mean is considered the most appropriate measure of central tendency. However, in cases where the data is numerical and contains extreme values or outliers, the median becomes the preferred choice. On the other hand, when dealing with categorical data, the mode is the preferred measure of central tendency.\n",
        "\n",
        "After analyzing the distribution and outliers in the data, we have identified the most appropriate measures of central tendency for imputing the null values in the columns as follows:\n",
        "\n",
        "\"Education\" , \"BPMeds\" -> mode: As \"education\" and \"BPMeds\" is a categorical variable, the mode is the most appropriate measure of central tendency. The mode represents the most frequently occurring value in the distribution and can provide insight into the most common level of education in the dataset.\n",
        "\n",
        "\"lGucose\",\"TotChol\", \"cigsPerDay\", \"BMI\", \"HeartRate\" -> median: Since this are numerical, continuous variable that contain extreme values or outliers, we have chosen the median as the appropriate measure of central tendency. The median is less sensitive to extreme values than the mean and provides a representative value for the central tendency of the distribution."
      ],
      "metadata": {
        "id": "aiSCBiSQe_wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing missing values with median or mode\n",
        "data_set.fillna({'glucose': data_set['glucose'].median(),\n",
        "           'education': data_set['education'].mode()[0],\n",
        "           'BPMeds': data_set['BPMeds'].mode()[0],\n",
        "           'totChol': data_set['totChol'].median(),\n",
        "           'cigsPerDay': data_set['cigsPerDay'].median(),\n",
        "           'BMI': data_set['BMI'].median(),\n",
        "           'heartRate': data_set['heartRate'].median()}, inplace=True)"
      ],
      "metadata": {
        "id": "8IY21jDje_Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.isna().sum()"
      ],
      "metadata": {
        "id": "RZJJ_Fhhh4s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized a combination of median and mode imputation techniques to address the issue of missing values. Specifically, for columns such as glucose, totChol, cigsPerDay, BMI, and heartRate, the missing values were replaced with the median of the non-missing values. On the other hand, for columns such as education and BPMeds, the missing values were replaced with the mode, i.e., the most frequently occurring value, of the non-missing values. Median imputation is generally preferred for continuous variables due to its robustness to outliers when compared to mean imputation. On the other hand, mode imputation is commonly utilized for categorical variables or variables with discrete values that have a limited number of possible values."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 1 -Which age group is more susceptible to developing coronary heart disease?"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Set the figure size\n",
        "fig, ax = plt.subplots(figsize=(4,2))\n",
        "# Create a boxplot to compare the age distribution of patients by sex and CHD risk level\n",
        "sns.boxplot(x=\"sex\", y=\"age\", hue=\"TenYearCHD\", data= data_set, ax=ax)\n",
        "# Set the title and labels\n",
        "ax.set_title(\"Age Distribution of Patients by Sex and CHD Risk Level\")\n",
        "ax.set_xlabel(\"Sex\")\n",
        "ax.set_ylabel(\"Age\")\n",
        "# Adding a legend with appropriate labels\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, [\"No Risk\", \"At Risk\"], loc=\"best\")\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The displayed boxplot provides a graphical representation of the distribution of ages among patients based on their sex and risk level for CHD (coronary heart disease). The boxplot is a useful tool to investigate the potential relationship between age, sex, and CHD risk level within the dataset. The authors likely selected this chart to explore patterns and trends among these variables more intuitively."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data analysis reveals a prominent contrast in the age distribution of patients based on their CHD (coronary heart disease) risk level. Specifically, patients who are at risk for CHD tend to be older than those who are not at risk, regardless of sex. This observation underscores the importance of age as a potential risk factor for CHD development."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information extracted from this chart could prove beneficial for healthcare-related businesses. For instance, companies that manufacture medical products or medications aimed at combating CHD could leverage these findings to target a specific patient group - those who are at higher risk or of advanced age. Nonetheless, it is worth noting that the chart's insights should not be the sole basis for making business decisions. Further investigation would be necessary to gain a comprehensive understanding of the interplay between age, sex, CHD risk level, and other pertinent factors. It's important to mention that there are no indications from this chart that suggest any negative growth."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  2 -Do patients taking medication for blood pressure have a higher risk of developing coronary heart disease?)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Compute the cross-tabulation of BP medication and CHD risk\n",
        "ct = pd.crosstab(data_set['BPMeds'],data_set['TenYearCHD'], normalize='index')\n",
        "# Plot a stacked bar chart\n",
        "ct.plot(kind='bar', stacked=True, figsize=(4,2))\n",
        "plt.title('Relationship between BP Medication and CHD Risk')\n",
        "plt.xlabel('BP Medication')\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylabel('Proportion')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart depicts a stacked bar chart that visualizes the distribution of CHD risk levels among patients who take medication for blood pressure and those who do not. The purpose of this chart may have been to explore the potential association between BP medication use and CHD risk in the dataset."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that there is a difference in the CHD risk between patients who take medication for blood pressure and those who do not. Patients who take medication for blood pressure seem to have a higher risk of CHD compared to those who do not take such medication. The chart implies that BP medication use may play a critical role in determining CHD risk in this dataset."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce the risk of CHD, companies that produce BP medication or other treatments for hypertension may want to consider targeting patients who have high blood pressure, especially those who are at risk for CHD, whether or not they are currently taking medication for blood pressure. This strategy may help identify patients who would benefit from more intensive treatment to lower their risk of developing CHD."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 3 -  Is a person who has had a stroke more susceptible to coronary heart disease?"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.countplot(x=data_set['prevalentStroke'], hue=data_set['TenYearCHD'])\n",
        "plt.title('Are people who had a stroke earlier more prone to CHD?')\n",
        "plt.legend(['No Risk', 'At Risk'], loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The presented countplot compares the prevalence of CHD risk level in patients who have experienced a stroke versus those who have not, potentially aiming to explore a potential correlation between having a stroke and an increased vulnerability to CHD."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the presented chart, it seems that individuals who have experienced a stroke have a greater likelihood of being susceptible to CHD than those who have not. More precisely, the countplot indicates that the percentage of patients at risk for CHD is higher in the group with a history of stroke when compared to the group without. Such results imply that having a stroke might be a contributing factor in the development of CHD in the analyzed dataset"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The knowledge obtained from this countplot could be beneficial for healthcare-related businesses that offer products or services for stroke or CHD. For instance, companies that manufacture medications or treatments for either condition may consider directing their focus towards patients who have previously suffered a stroke, as they seem to have a higher risk of developing CHD. Healthcare providers may also find this information relevant, as they can use it to screen patients who have had a stroke for potential CHD risk and provide adequate preventative measures or treatments accordingly."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 4 - Does having hypertension increase the risk of developing coronary heart disease?"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.countplot(x=data_set['prevalentHyp'], hue=data_set['TenYearCHD'])\n",
        "plt.title('Are hypertensive patients at more risk of CHD?')\n",
        "plt.legend(title='CHD Risk', labels=['No Risk', 'At Risk'])\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of selecting this chart was to illustrate the correlation between prevalent hypertension and the likelihood of developing coronary heart disease within the dataset"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that individuals who have pre-existing hypertension are at a higher risk of developing coronary heart disease compared to those who do not have hypertension. Specifically, the proportion of patients who are susceptible to CHD is comparable between individuals with prevalent hypertension and those without it."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information gained from this chart can be useful for healthcare professionals and businesses in identifying patients with hypertension who are at a higher risk of developing or advancing coronary heart disease. This knowledge can aid in providing appropriate evaluation, monitoring, and management to prevent or slow down the progression of the disease. There is no evidence in this chart to suggest any negative growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 5 - Which Gender has More Risk for CHD?"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.countplot(x='sex', hue='TenYearCHD', data= data_set)\n",
        "plt.title('Frequency of CHD cases by gender')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot displayed provides a visual representation of the incidence of CHD (coronary heart disease) cases among males and females in the dataset. The authors probably selected this chart to determine whether gender is a potential factor that influences CHD risk levels in the data. By examining the frequency of CHD cases across genders, researchers could explore the relationship between gender and CHD risk levels.."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates that CHD (coronary heart disease) cases are more prevalent among men than women in the dataset. Nevertheless, this difference is not significant, as the number of CHD cases is relatively comparable between genders. Additionally, the chart demonstrates that women have a higher count of no risk for CHD than men."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights obtained from this chart could potentially assist healthcare-related businesses in their decision-making process. For instance, firms that manufacture medical devices or medications for CHD might want to consider targeting both men and women, though they may want to pay more attention to men, who seem to be at a relatively higher risk of developing CHD in this dataset."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 6 - Did Smokers have more Risk of CHD or not?"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.countplot(x='is_smoking', hue='TenYearCHD', data= data_set)\n",
        "plt.title('Frequency of CHD cases by smoking habit')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot depicts the frequency of CHD (coronary heart disease) cases among smokers and non-smokers in the dataset. The authors probably selected this chart to understand the potential relationship between smoking and CHD risk levels in the data. By examining the incidence of CHD cases across smokers and non-smokers, researchers could explore the effect of smoking on CHD risk."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart indicates that patients who smoke are potentially more susceptible to CHD in this dataset. In particular, a larger proportion of patients who smoke are at risk of CHD when compared to non-smokers. The results imply that smoking may be a significant factor in determining CHD risk levels in this dataset."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the chart provides insights into the relationship between smoking and the risk of CHD in this dataset, it does not provide any information on negative growth. Furthermore, this chart alone may not provide enough information for businesses to make decisions, as other factors such as age and lifestyle also play a role in determining the risk of CHD. It is important to note that the dataset used for this chart may not be fully representative of the general population, which could limit the applicability of the insights gained from this chart."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 7 - How much smoking affect CHD?"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(14,6))\n",
        "sns.countplot(x= data_set['cigsPerDay'],hue= data_set['TenYearCHD'])\n",
        "plt.title('How much smoking affect CHD?')\n",
        "plt.legend(['At Risk', 'No Risk'])\n",
        "plt.show()\n",
        "     \n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this chart is to examine the potential relationship between smoking intensity, measured by the number of cigarettes smoked per day, and the risk of CHD in this dataset. The countplot visualizes the frequency of CHD cases for each category of the number of cigarettes smoked per day. By examining the count of CHD cases in each category, it may be possible to identify any patterns or trends that suggest a relationship between smoking intensity and CHD risk in this dataset."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates that smoking intensity may be related to CHD risk in this dataset. Patients who smoke more cigarettes per day, or do not smoke, appear to be at higher risk for CHD than those who smoke fewer cigarettes per day. The proportion of patients at risk for CHD is higher among those who smoke 20 or more cigarettes per day than among those who smoke fewer cigarettes per day. These insights highlight the potential importance of smoking intensity in determining the risk of CHD and may be useful for businesses that provide healthcare services or products targeting smoking cessation or reducing smoking intensity. However, it's important to note that this chart alone may not provide enough information to make conclusive business decisions, and further analysis would be required to fully understand the relationship between smoking intensity and CHD risk."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses that provide smoking cessation aids or medications for CHD may find it beneficial to target heavy smokers, as the chart suggests that they are at higher risk for CHD in this dataset."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 8 - How much Diabetes affect CHD?"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.barplot(x=data_set['diabetes'], y=data_set['TenYearCHD'], hue=data_set['TenYearCHD'], estimator=lambda x: len(x) / len(data_set) * 100)\n",
        "plt.title('Proportion of patients with and without diabetes at CHD risk')\n",
        "plt.xlabel('Diabetes')\n",
        "plt.ylabel('Percentage')\n",
        "plt.legend(title='CHD Risk', labels=['No Risk', 'At Risk'])\n",
        "plt.show()\n",
        "     \n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart was selected to illustrate the ratio of patients in the dataset who have diabetes and those who do not, and who are susceptible to developing coronary heart disease."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the chart, individuals who have diabetes are at a higher risk of developing coronary heart disease in comparison to those who do not have diabetes."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gleaned from this chart can assist healthcare professionals and businesses in identifying patients with diabetes who are at a heightened risk of developing or exacerbating coronary heart disease. This knowledge can aid in providing appropriate evaluation, monitoring, and management to prevent or slow down the progression of the disease."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 9 - How much Cholestrol affect CHD?"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.boxplot(x='TenYearCHD', y='totChol', data=data_set)\n",
        "plt.title('How much Cholestrol affect CHD?')\n",
        "plt.legend(['At Risk', 'No Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The particular box plot was selected to address the query of whether there exists a correlation between an individual's total cholesterol levels and their likelihood of developing coronary heart disease.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per the box plot, on average, patients who are susceptible to developing coronary heart disease have marginally higher total cholesterol levels than those who are not at risk. However, there is a degree of overlap in the cholesterol level range between the two groups."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The knowledge acquired from this box plot can aid healthcare providers in acknowledging the influence of total cholesterol levels on their patients' risk of developing coronary heart disease. Identifying patients with elevated cholesterol levels can enable healthcare providers to initiate appropriate interventions to mitigate their risk of developing the disease. This can have a positive impact on the patients' health outcomes and result in long-term cost savings for healthcare providers."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart 10 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "_6YgzjaFfqiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "data_set.corr()"
      ],
      "metadata": {
        "id": "Y2AlW6vZfxYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(15,15))\n",
        "correlation = data_set.corr()\n",
        "sns.heatmap((correlation), annot=True, cmap=sns.color_palette(\"YlOrRd\", as_cmap=True))\n",
        "     "
      ],
      "metadata": {
        "id": "-6O51Cr0f3sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "4qBRKCXzgCWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use the Correlation Heatmap because it allows for a comprehensive visualization of the correlations between all pairs of features in a dataset. The use of colors to represent the magnitude of the correlation coefficient makes it easy to quickly identify highly correlated features and patterns within the data."
      ],
      "metadata": {
        "id": "V8TmoTZogCJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "pHuRTXK4gB9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Correlation Heatmap shows the pairwise correlation between all numerical features in the dataset.\n",
        "\n",
        "1.The correlation chart highlights that there is a significant positive correlation of 22% between age and TenYearCHD, which suggests that age may be an important predictor of CHD risk.\n",
        "\n",
        "2.Based on the heatmap, it appears that age, systolic blood pressure, and diastolic blood pressure are strongly correlated with the TenYearCHD target variable.\n",
        "\n",
        "3.Moreover, the heatmap shows a moderate positive correlation of 78% between systolic and diastolic blood pressure.\n",
        "\n",
        "4.In addition, diabetes and glucose appear to be positively correlated by 61%.\n",
        "\n",
        "5.Furthermore, prevalent hypertension is highly correlated with systolic and diastolic blood pressure, by 70% and 61% respectively.\n",
        "\n",
        "6.Lastly, the chart also indicates a negative correlation of 17% and 19% between age and education, and age and cigarettes per day, respectively."
      ],
      "metadata": {
        "id": "0Hc2chj3gB1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "sns.pairplot(data_set[continous_variable])"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot is a valuable tool to explore the associations between the continuous variables in a dataset. It provides an efficient way to detect linear or non-linear relationships between the variables, and also allows the identification of potential outliers or anomalies in the data."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot provides insights into the pairwise relationships between the continuous variables. Positive correlations are observed between certain variables, such as age and systolic blood pressure, as well as between BMI and glucose levels. Linear correlations are also observed between systolic blood pressure and diastolic blood pressure. Additionally, a weak positive correlation is observed between cigsPerDay and sysBP. However, no clear linear relationship is observed between any of the variables and the target variable, TenYearCHD."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Higher total cholesterol levels are associated with an increased risk of CHD.\n",
        "\n",
        "2.Diabetic patients are at a higher risk of developing CHD than non-diabetic patients.\n",
        "\n",
        "3.Individuals above the age of 50 are more likely to be at risk of TenYearCHD."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement 1 - Higher total cholesterol levels are associated with an increased risk of CHD."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis - There is no difference in the mean total cholesterol levels between the two groups\n",
        "\n",
        "Alternate Hypothesis - There is a significant difference in the mean total cholesterol levels between the two groups."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Import the required statistical test module from scipy\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separate the dataset into two groups based on CHD status\n",
        "chd = data_set[data_set['TenYearCHD'] == 1] # Patients with CHD\n",
        "no_chd = data_set[data_set['TenYearCHD'] == 0] # Patients without CHD\n",
        "\n",
        "# Perform a two-sample t-test to compare the mean total cholesterol levels of the two groups\n",
        "t_test, p_value = stats.ttest_ind(chd['totChol'], no_chd['totChol'], equal_var=False)\n",
        "\n",
        "# Print the calculated t-statistic and p-value\n",
        "print('t_test= {}, p_value= {}'.format(t_test, p_value))\n",
        "\n",
        "# Determine if the null hypothesis should be rejected based on the p-value\n",
        "if p_value < 0.05:\n",
        "    print('Reject the null hypothesis')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_value)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value of 5.310852329016078e-07 is statistically significant as it is much smaller than the significance level of 0.05.\n",
        "\n",
        "Rejecting the null hypothesis, we can conclude that there is a significant difference in total cholesterol levels between the two groups (CHD and no CHD).\n",
        "\n",
        "This finding suggests that higher total cholesterol levels are positively associated with an increased risk of CHD.\n",
        "\n",
        "The t-statistic of 5.065 further confirms this result as it indicates a significant difference between the means of the two groups in terms of total cholesterol levels."
      ],
      "metadata": {
        "id": "4wjvdbMm-pEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the significance of the difference in mean total cholesterol levels between the two groups, a two-sample t-test was conducted. This statistical test was applied to compare the mean values of total cholesterol in the group with CHD and the group without CHD."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hypothesis \"Higher total cholesterol levels are associated with an increased risk of CHD.\" led to the use of a two-sample t-test, which is appropriate for comparing the mean total cholesterol levels of two independent groups: those with CHD and those without. Since CHD status is a dichotomous outcome variable, the means of the two groups are compared to assess if there is a significant difference in total cholesterol levels. The two-sample t-test assumes that the data is normally distributed and that the variances of the two groups are unequal."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement 2 - Diabetic patients are at a higher risk of developing CHD than non-diabetic patients."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: There is no significant difference in the risk of developing CHD between diabetic and non-diabetic patients.\n",
        "\n",
        "Alternative hypothesis: Diabetic patients are at a higher risk of developing CHD than non-diabetic patients."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Import the required statistical test module from scipy\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separate the dataset into two groups based on diabetic status\n",
        "diabetic = data_set[data_set['diabetes'] == 1]\n",
        "non_diabetic = data_set[data_set['diabetes'] == 0]\n",
        "\n",
        "# Perform a two-sample t-test to compare the mean TenYearCHD rates of the two groups\n",
        "t_stat, p_value = stats.ttest_ind(diabetic['TenYearCHD'], non_diabetic['TenYearCHD'], equal_var=False)\n",
        "\n",
        "print('t_stat=%.3f, p_val=%.3f' % (t_stat, p_value))\n",
        "if p_value > 0.05:\n",
        "    print('Accept Null Hypothesis')\n",
        "else:\n",
        "    print('Reject Null Hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_value)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-statistic represents the difference in means between diabetic and non-diabetic patients, standardized by the standard error of the difference. Meanwhile, the p-value indicates the probability of observing a difference in means as large or larger than what was found in the data, assuming that the null hypothesis is true.\n",
        "\n",
        "Since the obtained p-value of 0.000 is smaller than the significance level of 0.05, it suggests that the likelihood of seeing such a difference in means by chance is quite low. Hence, we reject the null hypothesis and conclude that diabetic patients have a greater risk of developing CHD than non-diabetic patients."
      ],
      "metadata": {
        "id": "u9QI_uS5gvz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was used to obtain the p-value for the hypothesis \"Diabetic patients are at a higher risk of developing CHD than non-diabetic patients.\""
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose to use the two-sample t-test since we are interested in comparing the means of two independent groups (diabetic vs non-diabetic) in relation to the binary outcome variable of CHD risk. This test is suitable for this type of analysis as it enables us to examine whether there is a significant difference between the means of the two groups. Moreover, due to the relatively large sample sizes of both groups, the t-test is considered a robust and dependable test to use."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement 3 - Individuals above the age of 50 are more likely to be at risk of TenYearCHD."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis - Age has no effect on the risk of TenYearCHD.\n",
        "\n",
        "Alternative hypothesis - Patients over 50 years of age have a higher risk of TenYearCHD than those who are under 50 years of age."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.stats.proportion as smp\n",
        "\n",
        "# Separate the dataset into two groups based on age\n",
        "above_50 = data_set[data_set['age'] > 50]\n",
        "below_50 = data_set[data_set['age'] <= 50]\n",
        "\n",
        "# Calculate the proportion of patients with TenYearCHD in each group\n",
        "prop_above_50 = above_50['TenYearCHD'].mean()\n",
        "prop_below_50 = below_50['TenYearCHD'].mean()\n",
        "\n",
        "# Perform a one-tailed z-test to compare the proportions of the two groups\n",
        "z_score, p_value = smp.proportions_ztest([prop_above_50 * len(above_50), prop_below_50 * len(below_50)], [len(above_50), len(below_50)], alternative='larger')\n",
        "\n",
        "print('z_score=%.3f, p_val=%.3f' % (z_score, p_value))\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print('Reject Null Hypothesis')\n",
        "else:\n",
        "    print('Accept Null Hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_value)\n",
        "     "
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the test reveal that the likelihood of chance being responsible for the variation in the proportion of TenYearCHD risk between patients aged above 50 and those below 50 is very low. Therefore, we reject the null hypothesis and conclude that patients who are above 50 years of age have a significantly higher risk of TenYearCHD than those who are below 50 years of age."
      ],
      "metadata": {
        "id": "nQJAunppNZMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a one-tailed Z-test to compare the proportions of patients with TenYearCHD above and below 50 years of age."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine if patients above 50 years of age have a higher risk of TenYearCHD than those below 50 years, a one-tailed z-test was selected. This test was chosen because we are specifically interested in whether the proportion of TenYearCHD in the group above 50 years is greater than the proportion in the group below 50 years. A z-test is appropriate in this case because we have a large sample size, and it allows for the comparison of proportions between two groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "data_set.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no remaining null values in our dataset as we have already processed and handled them in data wrangling."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16,8))\n",
        "axes = axes.flatten()\n",
        "for ax, col in zip(axes, continous_variable):\n",
        "    sns.boxplot(data_set[col], ax=ax)\n",
        "    ax.set_title(col.title(), weight='bold')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outliers_df(data_set, continuous_features):\n",
        "    outlier_df = pd.DataFrame(columns=['feature', 'lower_limit', 'upper_limit',\n",
        "                                       'IQR', 'total_outliers', 'percentage_outliers(%)'])\n",
        "    for feature in continuous_features:\n",
        "        values = data_set[feature]\n",
        "        q1, q2, q3 = values.quantile([0.25, 0.5, 0.75])\n",
        "        iqr = q3 - q1\n",
        "        Lower_limit = q1 - 1.5 * iqr\n",
        "        Upper_limit = q3 + 1.5 * iqr\n",
        "        outliers = values[(values < Lower_limit) | (values > Upper_limit)]\n",
        "        total_outliers = len(outliers)\n",
        "        percentage_outliers = round(total_outliers * 100 / len(values), 2)\n",
        "        outlier_df = outlier_df.append({'feature': feature,\n",
        "                                        'lower_limit': Lower_limit,\n",
        "                                        'upper_limit': Upper_limit,\n",
        "                                        'IQR': iqr,\n",
        "                                        'total_outliers': total_outliers, \n",
        "                                        'percentage_outliers(%)': percentage_outliers}, \n",
        "                                        ignore_index=True)\n",
        "    return outlier_df.sort_values(by=['percentage_outliers(%)'], ascending=False)\n",
        "     "
      ],
      "metadata": {
        "id": "RML-ZGKknqQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_df(data_set,continous_variable)"
      ],
      "metadata": {
        "id": "_LJHiePqTJYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the potential presence of critical patients among the outliers, moving all of them into the 25-75 interquartile range may not be a suitable approach for this dataset.\n",
        "\n",
        "There are various methods available for treating outliers in a dataset, including Winsorization, Robust statistical methods, Removing outliers, and Transformation, which involves applying mathematical functions such as logarithmic, square root, or reciprocal to the data to normalize its distribution and mitigate the impact of outliers.\n",
        "\n",
        "Regarding the third sentence, a possible rephrasing could be: \"Although Removing outliers is one of the methods commonly used to deal with outliers, it can lead to a loss of information and a decrease in the sample size.\n",
        "\n",
        "In light of the potential drawbacks of Removing outliers and the importance of preserving the sample size and information, we opt to employ transformation as a means of dealing with outliers in this dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "z7B6Ctc7T3MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying transformation for treating outlier \n",
        "data_set[continous_variable] = np.log(data_set[continous_variable] +1 )"
      ],
      "metadata": {
        "id": "hYLgaxQIUEnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used LOG TRANSFORMATION method to remove outlier from the dataset.\n",
        "\n",
        "I used this method because it is a statistical model and easy to implement yet it shows effective results.\n",
        "\n",
        "This transformaton can also help to normalize the distribution of the data and make it more symmetrical."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set['sex'] = pd.get_dummies(data_set['sex'], drop_first=True)\n",
        "data_set['is_smoking'] = pd.get_dummies(data_set['is_smoking'], drop_first=True)"
      ],
      "metadata": {
        "id": "90l43VPvVTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.info()"
      ],
      "metadata": {
        "id": "CDzFb-I2aiKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since machine learning algorithms typically require input in a numerical format, it is necessary to convert categorical variables into numerical ones. Therefore, we are using one-hot encoding to transform the non-numeric variables 'sex' and 'is_smoking' into binary values (0 or 1).\n",
        "\n",
        "The get_dummies() function from the pandas library is being utilized to generate dummy variables that represent the categories of each variable as individual binary columns.\n",
        "\n",
        "True parameter is being employed to prevent multicollinearity, which can arise when two dummy variables are highly correlated, by eliminating the first column of each set of dummy variables."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we observed from the correlation heatmap, there is a strong correlation between Systolic Blood Pressure and Diastolic Pressure. Therefore, we are generating a new feature that will indicate whether an individual has a blood pressure problem or not."
      ],
      "metadata": {
        "id": "F5Fj4TpjzuNi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOD9AaTkeQHw"
      },
      "source": [
        "After conducting a more in-depth analysis of heart-related problems, it was discovered that pulse pressure, defined as the difference between systolic and diastolic blood pressure, has a significant effect on CHD. Consequently, we can generate a new feature named PP (pulse pressure), which consolidates the systolic and diastolic blood pressure measurements into a solitary column."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding pulse pressure as a column\n",
        "data_set['pulsePressure'] = data_set['sysBP'] - data_set['diaBP']"
      ],
      "metadata": {
        "id": "DsigWThg0BGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how trip_duration and other features are related\n",
        "for col in data_set.describe().columns.tolist():\n",
        "    fig = plt.figure(figsize=(4, 2))\n",
        "    ax = fig.gca()\n",
        "    feature = data_set[col]\n",
        "    label = data_set['TenYearCHD']\n",
        "    correlation = feature.corr(label)\n",
        "    sns.scatterplot(x=feature, y=label, color=\"gray\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('TenYearCHD')\n",
        "    ax.set_title('TenYearCHD vs ' + col + ' - correlation: ' + str(correlation))\n",
        "    z = np.polyfit(data_set[col], data_set['TenYearCHD'], 1)\n",
        "    y_hat = np.poly1d(z)(data_set[col])\n",
        "    plt.plot(data_set[col], y_hat, \"r--\", lw=1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kNjXLB2C1Wff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(abs(round(data_set.corr(),3)), annot=True, linewidths=.5, fmt= '.1f',cmap='coolwarm',ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0US70o_GSXzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One method of analysis is correlation, which involves measuring the relationship between each feature and the target variable. Features that exhibit a strong correlation with the target variable are usually regarded as effective predictors and are thus chosen for selection."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Based on the heatmap analysis, we have observed a strong correlation between 'sysBP' and 'diaBP'. Since we have already computed a new feature called 'pulsePressure' from them, we are choosing to exclude both 'sysBP' and 'diaBP' from our analysis.\n",
        "\n",
        "2.The 'id' feature has been deemed less relevant to our analysis, and hence, we are dropping it.\n",
        "\n",
        "3.There is also a high correlation between the 'is_smoking' and 'cigsPerDay' columns, and therefore, one of them can be dropped if it is not contributing significantly to the target variable.\n",
        "\n",
        "4.The 'is_smoking' column has been assigned a value of 1 when the number of cigarettes smoked per day is greater than zero. Hence, both 'is_smoking' and the number of cigarettes smoked convey the same information. Therefore, we are removing the 'is_smoking' column."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.columns"
      ],
      "metadata": {
        "id": "y6NYzvZQjtyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating finl dataset\n",
        "data = data_set[['age', 'education', 'sex','cigsPerDay', 'BPMeds',\n",
        "               'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol',\n",
        "               'BMI', 'heartRate', 'glucose', 'pulsePressure', 'TenYearCHD']]"
      ],
      "metadata": {
        "id": "K7kNgU9Cjy_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for heatmap if anything remains to avoid multicollinearity \n",
        "plt.figure(figsize=(15,15))\n",
        "correlation = data.corr()\n",
        "sns.heatmap((correlation), annot=True, cmap=sns.color_palette(\"mako\", as_cmap=True))"
      ],
      "metadata": {
        "id": "u2WUN3eQkHTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Transformation is not required because we already did the transformation when treating outliers.\n",
        "\n",
        "But, we also updated our dataset, we added new feature as \"pulse pressure\".\n",
        "\n",
        "So, we will check for it, if it needs a transformation."
      ],
      "metadata": {
        "id": "AD8IvdqaTK0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data# Checking the distribution of pulse pressure \n",
        "plt.figure(figsize=(5,3))\n",
        "print(\"Before Applying Transformation\")\n",
        "sns.distplot(data_set['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to check whether feature is guassian or normal distributed\n",
        "# Q-Q plot\n",
        "stats.probplot(data_set['pulsePressure'],dist='norm',plot=pylab)\n",
        "     "
      ],
      "metadata": {
        "id": "YDw-aAQwUBcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 3 different copies to check the distribution of the variable\n",
        "dataset1 = data.copy()\n",
        "dataset2 = data.copy()\n",
        "dataset3 = data.copy()"
      ],
      "metadata": {
        "id": "LTlNFkwO9SkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use following Transformation Techniques:\n",
        "\n",
        "\n",
        "\n",
        "Logarithmic Transformation\n",
        "\n",
        "Reciprocal Trnasformation\n",
        "\n",
        "Square Root Transformation"
      ],
      "metadata": {
        "id": "apZu6mEXVJyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logarithmic Transformation**"
      ],
      "metadata": {
        "id": "rNoUh1DfV9PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1['pulsePressure']=np.log(dataset1['pulsePressure']+1)\n",
        "\n",
        "# Checking the distribution of continous variable \n",
        "plt.figure(figsize=(5,3))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.distplot(data_set['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')  "
      ],
      "metadata": {
        "id": "eTxM7Gw3V-Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Q plot\n",
        "stats.probplot(data_set['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "93okqJGaW7Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reciprocal Trnasformation**"
      ],
      "metadata": {
        "id": "7GMqesaeYr8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "dataset2['pulsePressure']=1/(dataset2['pulsePressure']+1)\n",
        "\n",
        "# Checking the distribution of continous variable \n",
        "plt.figure(figsize=(6,3))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.distplot(data_set['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "_-vCHNgNYsQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q-Q plot\n",
        "\n",
        "stats.probplot(data_set['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "ksrjiTX-aorR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square Root Transformation**"
      ],
      "metadata": {
        "id": "QBH5rc9FhrwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "dataset3['pulsePressure']=(dataset3['pulsePressure'])**(1/2)\n",
        "\n",
        "# Checking the distribution of continous variable \n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.distplot(data_set['pulsePressure'])\n",
        "plt.title('Distribution of pulsePressure')  "
      ],
      "metadata": {
        "id": "5RuRi-CXh41L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q-Q plot\n",
        "\n",
        "stats.probplot(data_set['pulsePressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "GRat0g76iY3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,we had already applied a logarithmic transformation to the data to eliminate outliers. However, since we generated the 'Pulse pressure' feature at a later stage, we had to determine the optimal transformation technique for this new feature as well. Our analysis revealed that the 'Pulse pressure' feature also required a logarithmic transformation."
      ],
      "metadata": {
        "id": "-92e95MTjzaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying Transform**"
      ],
      "metadata": {
        "id": "Met3utxjjzK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots it is clear that:\n",
        "\n",
        "Feature \"pulsePressure\" needs logarithmic transformation."
      ],
      "metadata": {
        "id": "ZgtJxl2wjzD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "## Logarithmic transformation\n",
        "data['pulsePressure']=np.log(data['pulsePressure']+1)"
      ],
      "metadata": {
        "id": "Wp2ALlKSkC3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "X = data.drop('TenYearCHD' , axis = 1)\n",
        "y = data[['TenYearCHD']]"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "# Creating object\n",
        "std_regressor= StandardScaler()\n",
        "     \n",
        "# Fit and Transform\n",
        "X = std_regressor.fit_transform(X)"
      ],
      "metadata": {
        "id": "reLGY2NZlGoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler is a data scaling technique that standardizes the data by ensuring that the mean is 0 and the standard deviation is 1. This method is frequently utilized in machine learning because it maintains the shape of the initial distribution and is appropriate for most machine learning algorithms, particularly those that utilize distance-based metrics. StandardScaler is also advantageous when the data features have vastly different scales, as it can aid in making the features more comparable."
      ],
      "metadata": {
        "id": "nqG-kG4hpThz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO it's not needed.\n",
        "\n",
        "For the cardiovascular risk prediction dataset, conducting dimensionality reduction is not obligatory. This is because the number of features in the dataset is comparatively small when contrasted with the number of samples. As a result, the possibility of overfitting is low. Furthermore, since the dataset is not very large, the time taken for training machine learning models would not be a major concern."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n"
      ],
      "metadata": {
        "id": "y9HMyjcR77hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(f'The shape of X_train is: {X_train.shape}')\n",
        "print(f'The shape of y_train is: {y_train.shape}')\n",
        "print(f'The shape of X_test is: {X_test.shape}')\n",
        "print(f'The shape of y_test is: {y_test.shape}')"
      ],
      "metadata": {
        "id": "v-2NkJyjplz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prevent overfitting and enhance model generalization performance, we partitioned the data such that 80% of it was used for training and the remaining 20% for testing. We employed the 'train_test_split' function from the scikit-learn library for this task, which is a popular technique for training and testing the model on distinct data samples.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_set.TenYearCHD.value_counts())"
      ],
      "metadata": {
        "id": "qTKJdVrNlszS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = data['TenYearCHD'].value_counts()\n",
        "labels =  [\"No\", \"Yes\"]\n",
        "plt.title('TenYearCHD Distribution', fontsize=10)\n",
        "\n",
        "# create pie chart \n",
        "plt.pie(count, labels=labels , autopct= \"%1.1f%%\",)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eOuAoPKE7hdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YES As shown in the pie chart, the target variable, which is the 10-year risk of coronary heart disease (CHD), is heavily imbalanced. Specifically, out of the total sample population, 84.9% or 2879 individuals are free from CHD risk, while only 15.1% or 511 individuals are at risk. Such a significant class imbalance in the data has the potential to lead to biased predictions, adversely affecting the performance of machine learning models. As a result, it is crucial to balance the data by employing appropriate techniques like undersampling or oversampling to enhance the accuracy and reliability of the models."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "nnjkFFXI7p62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Fit and apply SMOTE to the data\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Print the original and resampled dataset shapes\n",
        "print('Original dataset shape:', data_set.shape)\n",
        "print('Resampled dataset shape:', X_res.shape)\n",
        "\n",
        "# Count the number of samples in each class in the resampled dataset\n",
        "print('Class distribution in the resampled dataset:', y_res.value_counts())"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The shape of x_train is: {X_train.shape}')\n",
        "print(f'The shape of y_train is: {y_train.shape}')\n",
        "print(f'The shape of x_test is: {X_test.shape}')\n",
        "print(f'The shape of y_test is: {y_test.shape}')"
      ],
      "metadata": {
        "id": "6RFiR4ux9-Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the imbalanced dataset, I leveraged the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE is an oversampling technique that creates synthetic samples for the minority class by interpolating new instances between the existing ones. This method aids in balancing the class distribution and diminishing the bias towards the majority class in imbalanced datasets. As a result, the performance of machine learning models on imbalanced datasets may be improved."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating class for checking accuracy of model\n",
        "def checking_accuracy(y_train, y_test, train_preds, test_preds):\n",
        " \n",
        "  from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "  # Get the accuracy scores\n",
        "  train_accuracy = accuracy_score(y_train,train_preds)\n",
        "  test_accuracy = accuracy_score(y_test,test_preds)\n",
        "  print(\"The accuracy on train data is \", train_accuracy)\n",
        "  print(\"The accuracy on test data is \", test_accuracy)\n",
        "\n",
        "  from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "  # Get the precision scores\n",
        "  train_precision = precision_score(y_train,train_preds)\n",
        "  test_precision = precision_score(y_test,test_preds)\n",
        "  print(\"The precision on train data is \", train_precision)\n",
        "  print(\"The precision on test data is \", test_precision)\n",
        "\n",
        "  # Get the recall scores\n",
        "  train_recall = recall_score(y_train,train_preds)\n",
        "  test_recall = recall_score(y_test,test_preds)\n",
        "  print(\"The recall on train data is \", train_recall)\n",
        "  print(\"The recall on test data is \", test_recall)\n",
        "\n",
        "  # Get the f1 scores\n",
        "  train_f1 = f1_score(y_train,train_preds)\n",
        "  test_f1 = f1_score(y_test,test_preds)\n",
        "  print(\"The f1 on train data is \", train_f1)\n",
        "  print(\"The f1 on test data is \", test_f1)\n",
        "\n",
        "  # Get the f1 scores\n",
        "  train_roc_auc = roc_auc_score(y_train,train_preds)\n",
        "  test_roc_auc = roc_auc_score(y_test,test_preds)\n",
        "  print(\"The roc_auc on train data is \", train_roc_auc)\n",
        "  print(\"The roc_auc on test data is \", test_roc_auc)"
      ],
      "metadata": {
        "id": "Zo890_waADoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_train, y_test, train_preds, test_preds):\n",
        "  train_confusion_matrix = confusion_matrix(y_train, train_preds)\n",
        "  test_confusion_matrix = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "  labels = ['0', '1']\n",
        "  sns.heatmap(train_confusion_matrix, annot=True, cmap='Blues', ax=axes[0], fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "  axes[0].set_xlabel('Predicted labels')\n",
        "  axes[0].set_ylabel('True labels')\n",
        "  axes[0].set_title('Train Confusion Matrix')\n",
        "  sns.heatmap(test_confusion_matrix, annot=True, cmap='Blues', ax=axes[1], fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "  axes[1].set_xlabel('Predicted labels')\n",
        "  axes[1].set_ylabel('True labels')\n",
        "  axes[1].set_title('Test Confusion Matrix')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yUlTI_iGb1_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 1 - Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "\n",
        "# Fit the Algorithm\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_class_preds_lr = clf.predict(X_train)\n",
        "test_class_preds_lr = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "checking_accuracy(y_train,y_test,train_class_preds_lr,test_class_preds_lr)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for confusion matrix\n",
        "plot_confusion_matrix(y_train,y_test,train_class_preds_lr,test_class_preds_lr)"
      ],
      "metadata": {
        "id": "g2WGRYyY0bvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "lr = LogisticRegression()\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "para_grid = {'penalty': ['l1', 'l2'],\n",
        "              'C': [0.1, 1.0, 10.0],\n",
        "              'solver': ['liblinear', 'saga']}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(lr, para_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# get the best hyperparameters and print them\n",
        "best_param = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_param)\n",
        "# use the best hyperparameters to fit the model and make predictions\n",
        "lr_best = LogisticRegression(**best_param)\n",
        "# perform cross-validation on the model with the best hyperparameters\n",
        "cv_scores = cross_val_score(lr_best, X_train, y_train, cv=5)\n",
        "# fit the final model using all the training data and the best hyperparameters\n",
        "lr_best.fit(X_train, y_train)\n",
        "y_train_logistic_pred_cv = lr_best.predict(X_train)\n",
        "y_test_logistic_pred_cv  = lr_best.predict(X_test)\n",
        "y_score_logistic_pred_cv = lr_best.predict_proba(X_test)[:, 1]\n",
        "     "
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "checking_accuracy(y_train,y_test,y_train_logistic_pred_cv, y_test_logistic_pred_cv)"
      ],
      "metadata": {
        "id": "JU-7lrHM-2cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for confusion matrix\n",
        "plot_confusion_matrix(y_train,y_test,y_train_logistic_pred_cv, y_test_logistic_pred_cv)"
      ],
      "metadata": {
        "id": "RFmrX7Xi-2fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a powerful technique to fine-tune the hyperparameters of machine learning models. It exhaustively evaluates all possible combinations of hyperparameters and their corresponding values, enabling the selection of the best combination based on performance calculation. This results in increased accuracy and improved performance of the model."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized GridSearchCV to search for the optimal hyperparameters, hoping to improve the performance of our machine learning model. Despite the exhaustive search of all possible hyperparameter combinations, our test results showed little improvement, with a test accuracy of only 67.97%, test precision and recall of 66.55% and 69.27%, respectively, and an area under the curve (ROC AUC) of only 0.68.\n",
        "\n",
        "As a result, we have decided to explore other models such as Random Forest and XGBoost to enhance our model's accuracy and AUC score."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 2 - Random Forest Classifier"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# ML Model - 2  Implementation\n",
        "random_forest = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_rf_pred = random_forest.predict(X_train)\n",
        "y_test_rf_pred = random_forest.predict(X_test)"
      ],
      "metadata": {
        "id": "TVBA40bag3jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "checking_accuracy(y_train, y_test,y_train_rf_pred,y_test_rf_pred)\n",
        "plot_confusion_matrix(y_train, y_test,y_train_rf_pred,y_test_rf_pred)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "random_forest = RandomForestClassifier()\n",
        "param_grid = {'n_estimators': [100, 200, 300],\n",
        "              'max_depth': [5, 10, 15, None],\n",
        "              'min_samples_split': [2, 5, 10],\n",
        "              'min_samples_leaf': [1, 2, 4]}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# use the best hyperparameters to fit the model to the training data\n",
        "random_forest_best = RandomForestClassifier(**best_params)\n",
        "random_forest_best.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_train_rf_pred_gs = random_forest_best.predict(X_train)\n",
        "y_test_rf_pred_gs  = random_forest_best.predict(X_test)\n",
        "y_score_rf_pred_gs = random_forest_best.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checking_accuracy(y_train, y_test,y_train_rf_pred_gs,y_test_rf_pred_gs)\n",
        "plot_confusion_matrix(y_train, y_test,y_train_rf_pred_gs,y_test_rf_pred_gs)"
      ],
      "metadata": {
        "id": "b-V4vgJviRQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a potent technique that can be employed to optimize the hyperparameters of machine learning models. This method involves systematically exploring all the feasible hyperparameter combinations and their respective values to determine the optimal configuration that maximizes the model's performance, thus resulting in more precise outcomes."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890.Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890.Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890.Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890.Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890.Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890.Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890.Through the utilization of GridSearchCV, we were able to enhance the performance of our machine learning model by identifying the most appropriate hyperparameters. GridSearchCV systematically evaluates all the possible combinations of hyperparameters and determines the optimal values to maximize the model's performance, leading to the most precise outcomes.\n",
        "\n",
        "Following the hyperparameter tuning, we obtained the optimal parameter configuration, which included 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200. Although the training accuracy was 100%, we were able to improve the test accuracy from 83.07% to 88.89%.\n",
        "\n",
        "Moreover, we were able to enhance the ROC AUC score from 0.8311 to 0.8890."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assessing the performance of an ML model is essential to determine the accuracy of its predictions. To evaluate the model's effectiveness, we used various metrics, such as Accuracy, Precision, Recall, and ROC AUC score, to determine how closely the predicted values aligned with the actual values. Our findings revealed that the model accurately predicted the Ten Year CHD with an accuracy rate of around 88.89%. This level of accuracy is particularly noteworthy as the TenYearCHD variable directly influences business outcomes."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 3 - XGBoost Classifier"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "# ML Model - 3 Implementation\n",
        "xgb = XGBClassifier()\n",
        "# Fit the Algorithm\n",
        "xgb.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_train_xgb_pred = xgb.predict(X_train)\n",
        "y_test_xgb_pred = xgb.predict(X_test)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "checking_accuracy(y_train, y_test,y_train_xgb_pred, y_test_xgb_pred)\n",
        "plot_confusion_matrix(y_train, y_test, y_train_xgb_pred, y_test_xgb_pred)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'max_depth': [3, 5, 7],\n",
        "              'learning_rate': [0.01, 0.1, 0.3],\n",
        "              'n_estimators': [50, 100, 200]}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(xgb, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "# print the best hyperparameters\n",
        "print('Best hyperparameters:', grid_search.best_params_)\n",
        "# Predict on the model\n",
        "best_estimator = grid_search.best_estimator_\n",
        "y_train_xgb_pred_gs = best_estimator.predict(X_train)\n",
        "y_test_xgb_pred_gs  = best_estimator.predict(X_test)\n",
        "y_score_xgb_pred_gs = best_estimator.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checking_accuracy(y_train, y_test,y_train_xgb_pred_gs, y_test_xgb_pred_gs)\n",
        "plot_confusion_matrix(y_train, y_test, y_train_xgb_pred_gs, y_test_xgb_pred_gs)"
      ],
      "metadata": {
        "id": "mCvI8oYOSEwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To optimize the hyperparameters of our machine learning model, we utilized the powerful technique of GridSearchCV. This method is highly effective as it systematically evaluates all possible combinations of hyperparameters and their corresponding values, and selects the optimal combination based on performance metrics. Employing this approach leads to improved model performance and more precise outcomes."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized the exhaustive evaluation of all possible hyperparameter combinations offered by GridSearchCV to optimize our machine learning model's performance. This approach enabled us to identify the optimal hyperparameter values, leading to more precise outcomes and improved model performance.\n",
        "\n",
        "Following the hyperparameter tuning process, we determined the best parameters as 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200. As a result, the accuracy of our model significantly improved from 82.55% to 89.67%. Furthermore, we observed notable improvements in the Precision and Recall metrics, which increased to 92.69% and 85.61%, respectively. Additionally, the ROC AUC score improved to 0.8958, which is considered good."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 4 - K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "8xqk1t18lrF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# ML Model - 4 Implementation\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_knn_pred = knn.predict(X_train)\n",
        "y_test_knn_pred = knn.predict(X_test)"
      ],
      "metadata": {
        "id": "4qsD5fB9lxxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "6gk2BnL0mjrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "checking_accuracy(y_train, y_test, y_train_knn_pred, y_test_knn_pred)\n",
        "plot_confusion_matrix(y_train, y_test, y_train_knn_pred, y_test_knn_pred)"
      ],
      "metadata": {
        "id": "u-VaXFnImlgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "hA2qG4l4nPpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'n_neighbors': [3, 5, 7],\n",
        "              'weights': ['uniform', 'distance']}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# train the classifier with the best hyperparameters on the full training set\n",
        "knn_best = KNeighborsClassifier(**best_params)\n",
        "knn_best.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_knn_pred_gs  = knn_best.predict(X_test)\n",
        "y_train_knn_pred_gs = knn_best.predict(X_train)\n",
        "y_score_knn_pred_gs = knn_best.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "74QAI-HanTUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "checking_accuracy(y_train, y_test, y_train_knn_pred_gs, y_test_knn_pred_gs)\n",
        "plot_confusion_matrix(y_train, y_test, y_train_knn_pred_gs, y_test_knn_pred_gs)"
      ],
      "metadata": {
        "id": "lW50FwhqnkmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HKpojIk0n60U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the performance of our machine learning model, we employed GridSearchCV to optimize the hyperparameters. This technique exhaustively evaluates all feasible hyperparameter combinations and their respective values, selecting the optimal configuration to maximize model performance. By employing this approach, we achieved more precise results and significantly improved the model's performance."
      ],
      "metadata": {
        "id": "CxrRSEqon_Wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "eYFX44sioEGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the utilization of GridSearchCV, we optimized our machine learning model by exhaustively searching for the best hyperparameters among all possible combinations. This rigorous process enabled us to select the optimal values, resulting in a significant improvement in model performance.\n",
        "\n",
        "In the KNN model, we observed an increase in accuracy from 78.56% to 82.12%, a Precision of 74.02%, Recall of 97.69%, and ROC AUC score of 0.8246, which is higher after hyperparameter tuning. However, the ROC AUC score is lower than the previous model."
      ],
      "metadata": {
        "id": "vxAlyp9VoD-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 5 - Support Vector Machine Classifier (SVC)"
      ],
      "metadata": {
        "id": "kd0_jJGAoDzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "# ML Model - 5 Implementation\n",
        "svc = SVC(kernel='rbf', C=1, gamma='scale')\n",
        "\n",
        "# Fit the Algorithm\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_svc_pred = svc.predict(X_train)\n",
        "y_test_svc_pred = svc.predict(X_test)"
      ],
      "metadata": {
        "id": "PJSUkAktoC5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n"
      ],
      "metadata": {
        "id": "NmYm8loDpYjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checking_accuracy(y_train, y_test, y_train_svc_pred, y_test_svc_pred)\n",
        "plot_confusion_matrix(y_train, y_test, y_train_svc_pred, y_test_svc_pred)"
      ],
      "metadata": {
        "id": "zTav_ROKpe-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "6TxTXkZbpYgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "svc = SVC(probability=True)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'kernel': ['linear', 'rbf'],\n",
        "              'gamma': ['scale', 'auto']}\n",
        "# perform a grid search with 5-fold cross-validation to find the best hyperparameters\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# train the classifier with the best hyperparameters on the full training set\n",
        "svc_best = SVC(**best_params, probability=True)\n",
        "svc_best.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_svc_pred_gs = svc_best.predict(X_test)\n",
        "y_train_svc_pred_gs = svc_best.predict(X_train)\n",
        "y_score_svc_pred_gs = svc_best.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "Yg_Y6fvRqP_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checking_accuracy(y_train, y_test, y_train_svc_pred_gs, y_test_svc_pred_gs)\n",
        "plot_confusion_matrix(y_train, y_test, y_train_svc_pred_gs, y_test_svc_pred_gs)"
      ],
      "metadata": {
        "id": "UWE39GovqTOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "QeFSM7FEpYdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By utilizing GridSearchCV, we optimized the hyperparameters of our machine learning model to fine-tune it for optimal performance. This technique evaluates all feasible hyperparameter combinations and their respective values to identify the best configuration for maximizing model performance, resulting in more precise outcomes and improved model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "V3HNQTI7rMwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "WfXPWiO-rQXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To optimize the performance of our machine learning model, we utilized GridSearchCV to search for the best hyperparameters. This technique evaluates all possible combinations of hyperparameters and selects the optimal values to enhance model performance, leading to more precise outcomes.\n",
        "\n",
        "Upon performing hyperparameter tuning, we observed a modest improvement in our model's performance. The accuracy improved from 70.14% to 76.74%, precision increased from 68.65% to 73.30%, and recall rose from 71.58% to 82.42%. Additionally, we achieved an AUC ROC of 76.86%."
      ],
      "metadata": {
        "id": "4UoMdnZQrQUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here Predicting TenYearCHD considered as classification problem, where the goal is to predict a outcome variable (TenYearChd) based on one or more predictor variables.\n",
        "\n",
        "**Accuracy**: Accuracy is a commonly used metric to evaluate the performance of a classification model. It measures the percentage of correctly classified instances among all instances. A higher accuracy score indicates a better performance of the model in predicting the correct class for each instance.\n",
        "\n",
        "**Precision**: Precision is a metric that measures the proportion of true positive predictions among all positive predictions made by the model. It is calculated by dividing the number of true positives by the sum of true positives and false positives. A higher precision score indicates that the model has a lower rate of false positives, which is important in applications where false positives have a high cost.\n",
        "\n",
        "**Recall**: Recall, also known as sensitivity or true positive rate, is a metric that measures the proportion of true positive predictions among all instances that actually belong to the positive class. It is calculated by dividing the number of true positives by the sum of true positives and false negatives. A higher recall score indicates that the model has a lower rate of false negatives, which is important in applications where false negatives have a high cost.\n",
        "\n",
        "**AUC ROC**: The Area Under the Receiver Operating Characteristic Curve (AUC ROC) is a metric used to evaluate the performance of binary classification models. It measures the model's ability to distinguish between positive and negative classes at different probability thresholds. The AUC ROC score ranges between 0 and 1, with a score of 0.5 indicating a random model and a score of 1 indicating a perfect model. A higher AUC ROC score indicates a better performance of the model in distinguishing between positive and negative classes.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the classifiers\n",
        "classifiers = [ (\"Logistic Regression\", LogisticRegression()),\n",
        "                (\"Random Forest Classifier\", RandomForestClassifier()),\n",
        "                (\"XGB Classifier\", XGBClassifier()),\n",
        "                (\"KNN\", KNeighborsClassifier()),\n",
        "                (\"SVC\", SVC(probability=True)),]\n",
        "\n",
        "# iterate through classifiers and plot ROC curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "for name, classifier in classifiers:\n",
        "    classifier.fit(X_train, y_train)\n",
        "    y_score = classifier.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dn9u5ZOSavFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After cross validation and hyperparameter tuning"
      ],
      "metadata": {
        "id": "Ajtde_41TkGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing metrics in order to make dataframe\n",
        "# (after cross validation and hyperparameter tuning)\n",
        "Model = [\"Logistic Regression\", \"Random Forest Classifier\", \"XGBoost\", \"KNN\", \"SVC\"]\n",
        "Y_SCORE = [y_score_logistic_pred_cv, y_score_rf_pred_gs, y_score_xgb_pred_gs, \n",
        "           y_score_knn_pred_gs, y_score_svc_pred_gs]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'MODEL': Model, 'Y_SCORE': Y_SCORE}\n",
        "Metric_df = pd.DataFrame(data)\n",
        "\n",
        "# plot the ROC curves for each model\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, row in Metric_df.iterrows():\n",
        "    fpr, tpr, _ = roc_curve(y_test, row['Y_SCORE'])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{row['MODEL']} (AUC = {roc_auc:.2f})\", alpha=0.8)\n",
        "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SDw-HZe_Ykje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing metrics in order to make dataframe of metrics \n",
        "# (after cross validation and hyperparameter tuning)\n",
        "Model          = [\"Logistic Regression\", \"Random Forest Classifier\", \"XGBoost\", \"KNN\", \"SVC\"]\n",
        "Test_Accuracy  = [0.6797,0.8889,0.8967,0.8212,0.7674]\n",
        "Test_Precision = [0.6655,0.8796,0.9269,0.7402,0.7330]\n",
        "Test_Recall    = [0.6927,0.8952,0.8561,0.9769,0.8242]\n",
        "Test_f1_score  = [0.6788,0.8792,0.8901,0.8422,0.7759]\n",
        "Test_ROC_AUC   = [0.6800,0.8890,0.8958,0.8246,0.7686]\n",
        "# Create dataframe from the lists\n",
        "data = {\n",
        "        'Test_Accuracy'  : Test_Accuracy,\n",
        "        'Test_Precision' : Test_Precision,\n",
        "        'Test_Recall'    : Test_Recall,\n",
        "        'Test_f1_score'  : Test_f1_score,\n",
        "        'Test_ROC_AUC'   : Test_ROC_AUC}\n",
        "Metric_df = pd.DataFrame(data,index = Model)\n",
        "\n",
        "# Printing dataframe\n",
        "Metric_df   "
      ],
      "metadata": {
        "id": "3jzdGgKrTxVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a horizontal bar plot\n",
        "\n",
        "ax = Metric_df.plot(kind='bar', figsize=(9, 6), color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "ax.set_xlabel('Score')\n",
        "ax.set_ylabel('Model')\n",
        "ax.set_title('Comparison of Metrics for Different Models')"
      ],
      "metadata": {
        "id": "hRhmtzZ3NYht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating several models on the cardiovascular risk prediction project, it appears that the Random Forest Classifier and XGBoost models outperform the other models in terms of accuracy, precision, recall, and ROC AUC score. The Random Forest Classifier has an accuracy score of 0.8889, while the XGBoost model has an accuracy score of 0.8967, indicating that both models are suitable for use in real-time prediction systems. Additionally, the precision and recall scores of these models are high, indicating that they are good at correctly predicting positive and negative cases.\n",
        "\n",
        "However, the XGBoost model has slightly higher test accuracy, precision, and ROC AUC scores than the Random Forest Classifier, suggesting that it may be a better choice for predicting cardiovascular risk. Therefore, based on the evaluation results, the XGBoost model appears to be the better choice for predicting cardiovascular risk in this project, and we are selecting it as the best fit classification model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion From EDA"
      ],
      "metadata": {
        "id": "YOl5kiA5VuzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Age plays a significant role in determining the risk of CHD.\n",
        "\n",
        "2.Men have a higher likelihood of developing CHD compared to women.\n",
        "\n",
        "3.Smoking is a well-known risk factor for CHD, and the intensity of smoking further increases the risk.\n",
        "\n",
        "4.Patients with high blood pressure, stroke, and diabetes are at an elevated risk of developing CHD.\n",
        "\n",
        "5.Patients who have had a prevalent stroke or prevalent hypertension have a higher likelihood of being at risk for CHD.\n",
        "\n",
        "6.Patients with diabetes have an increased risk of CHD.\n",
        "\n",
        "7.Total cholesterol levels tend to be slightly higher in patients who are at risk for CHD.\n",
        "\n",
        "8.There exists a positive association between certain variables such as age and systolic blood pressure, as well as between BMI and glucose levels, with the risk of CHD.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion From Model Implementation"
      ],
      "metadata": {
        "id": "xLMhh1JVVqgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The Random Forest Classifier and XGBoost models outperformed the other four models tested in terms of accuracy, precision, and recall scores.\n",
        "\n",
        "2.Although the KNN model had a relatively high recall score, its accuracy and precision scores were lower compared to those of the Random Forest Classifier and XGBoost models.\n",
        "\n",
        "3.The SVC model had lower accuracy and ROC AUC scores, suggesting that it may not be the most suitable model for this specific classification problem.\n",
        "\n",
        "4.The XGBoost model had slightly higher test accuracy and precision scores than the Random Forest Classifier, and a higher ROC AUC score, indicating that it may be a better choice for predicting cardiovascular risk.\n",
        "\n",
        "5.Based on the presented results, the XGBoost model was chosen as the most suitable classification model for the cardiovascular risk prediction dataset, with an accuracy of 89.67%."
      ],
      "metadata": {
        "id": "9pZKr_xTVoaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}